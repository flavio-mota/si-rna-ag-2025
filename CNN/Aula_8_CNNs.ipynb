{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK7ecnb6pKzp"
      },
      "source": [
        "**Convolutional Neural Networks - CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeYcCO3HpKzt"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/flavio-mota/si-rna-ag-2025/blob/main/CNN/Aula_8_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFXIv9qNpKzt",
        "tags": []
      },
      "source": [
        "### Configurando o ambiente e bibliotecas\n",
        "\n",
        "Nesta primeira parte do notebook, vamos carregar as bibliotecas que usaremos ao longo da aula. Elas são responsáveis por tarefas como:\n",
        "\n",
        "- Carregar e manipular o conjunto de dados de imagens;\n",
        "- Construir a arquitetura da nossa rede neural convolucional (CNN);\n",
        "- Treinar o modelo e avaliar o desempenho;\n",
        "- Utilizar modelos pré-treinados em bases grandes como o ImageNet.\n",
        "\n",
        "A ideia é que, a partir deste ponto, tudo o que fizermos em termos de CNN seja construído sobre essas ferramentas. Não vamos nos aprofundar em cada função individual, mas é importante ter em mente que o `TensorFlow/Keras` é o “motor” por trás da criação e treinamento dos modelos que veremos hoje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqCwW7cMpKzw"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Piq5se2pKzx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaDoLQTpKzx"
      },
      "source": [
        "Vamos definir os tamanhos de fonte padrão para deixar as figuras mais bonitas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4TH3NbpKzx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsawKlapKzy"
      },
      "source": [
        "Estes códigos podem ser muito lentos sem uma GPU, então vamos garantir que haja uma, ou então emitir um aviso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekxzo6pOpKzy"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"Nenhuma GPU foi detectada. Redes neurais podem ser muito lentas sem uma GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Acesse Runtime > Alterar runtime e selecione um acelerador de hardware de GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oid44Xx-pKz6"
      },
      "source": [
        "# Arquiteturas de CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELZe7PLfpKz6"
      },
      "source": [
        "### O dataset Fashion-MNIST\n",
        "\n",
        "Para estudar redes convolucionais, vamos utilizar o conjunto de dados **Fashion-MNIST**. Ele é uma evolução do clássico MNIST: em vez de dígitos escritos à mão, temos imagens de peças de roupa (camisetas, calçados, bolsas etc.).\n",
        "\n",
        "Algumas características importantes:\n",
        "\n",
        "- As imagens são em escala de cinza, com tamanho 28×28 pixels;\n",
        "- Cada imagem pertence a uma de 10 classes diferentes;\n",
        "- Possui 60.000 imagens para treino e 10.000 para teste.\n",
        "\n",
        "Esse dataset é interessante porque é simples o suficiente para rodar em sala de aula, mas já é mais desafiador do que o MNIST original, o que nos permite ver melhor a vantagem de usar CNNs em relação a redes totalmente conectadas (MLPs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGpOfQlYrAE4"
      },
      "source": [
        "### Pré-processamento: por que normalizar as imagens?\n",
        "\n",
        "Antes de enviar as imagens para a rede, precisamos fazer um pequeno pré-processamento. Os valores dos pixels no Fashion-MNIST vão de 0 a 255. Quando dividimos tudo por 255, trazemos esses valores para o intervalo [0, 1].\n",
        "\n",
        "Essa normalização ajuda a rede de várias maneiras:\n",
        "\n",
        "- Deixa a escala dos dados mais estável para o processo de otimização;\n",
        "- Facilita o aprendizado dos pesos, já que os gradientes tendem a ficar em uma faixa mais “controlada”;\n",
        "- Costuma acelerar a convergência do treinamento.\n",
        "\n",
        "Além disso, aqui também ajustamos o formato das imagens (por exemplo, adicionando o canal “1” para indicar que são imagens em tons de cinza), de forma que a CNN consiga interpretar corretamente a estrutura dos dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IXwgw_0pKz6"
      },
      "outputs": [],
      "source": [
        "# código extra – carrega o conjunto de dados MNIST, adiciona o eixo dos canais às entradas,\n",
        "# dimensiona os valores para o intervalo de 0 a 1 e divide o conjunto de dados\n",
        "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
        "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
        "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Wk2RkmrDvf"
      },
      "source": [
        "### Construindo a arquitetura da CNN\n",
        "\n",
        "Agora vamos, de fato, definir a arquitetura da nossa **Convolutional Neural Network**. A grande diferença em relação a uma MLP é que aqui aproveitamos a estrutura espacial da imagem: ao invés de “achatar” tudo logo no início, usamos camadas que varrem a imagem com filtros (kernels) para extrair padrões locais.\n",
        "\n",
        "A arquitetura que vamos usar segue uma lógica bem comum em redes convolucionais simples:\n",
        "\n",
        "- Camadas **convolucionais**: aplicam filtros que detectam padrões como bordas, texturas e pequenos detalhes;\n",
        "- Camadas de **pooling** (como MaxPooling): reduzem a resolução espacial, mantendo as características mais importantes e diminuindo o número de parâmetros;\n",
        "- Camada **Flatten**: transforma o mapa de características em um vetor;\n",
        "- Camadas **densas** ao final: combinam essas características extraídas para realizar a classificação em uma das 10 classes.\n",
        "\n",
        "Essa é uma CNN relativamente pequena, mas já é suficiente para mostrar, na prática, como convoluções melhoram o desempenho em tarefas de visão computacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34upiak4pKz6"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
        "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "model = tf.keras.Sequential([\n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=128, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=64, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuRs2tGkrHFO"
      },
      "source": [
        "### Compilando e treinando a rede convolucional\n",
        "\n",
        "Com a arquitetura definida, o próximo passo é **compilar** e **treinar** o modelo. Ao compilar, definimos três elementos principais:\n",
        "\n",
        "- A **função de perda**: aqui usamos `sparse_categorical_crossentropy`, adequada para problemas de classificação com múltiplas classes e rótulos inteiros;\n",
        "- O **otimizador**: utilizaremos o `Adam`, que é um método de otimização adaptativo muito utilizado em deep learning;\n",
        "- As **métricas**: vamos acompanhar especialmente a acurácia, que indica a porcentagem de acertos na classificação.\n",
        "\n",
        "Durante o treinamento (`model.fit`), a rede ajusta os pesos dos filtros e das camadas densas, tentando minimizar a função de perda. Também usamos um conjunto de validação para observar se o modelo está generalizando bem ou começando a sofrer overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZbWeIBYpKz6"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "score = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:10]  # Finge que temos novas imagens\n",
        "y_pred = model.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGNEIcC7nTrR"
      },
      "source": [
        "### Por que utilizar modelos pré-treinados?\n",
        "\n",
        "Treinar redes convolucionais profundas do zero exige muito poder computacional e grandes volumes de dados. Para muitas aplicações práticas, isso é inviável ou desnecessário. Uma alternativa é aproveitar modelos que já foram treinados em bases enormes, como a ImageNet, e reutilizar esse conhecimento.\n",
        "\n",
        "Esses modelos pré-treinados aprenderam filtros capazes de detectar desde padrões simples (bordas, texturas) até conceitos mais complexos (partes de objetos, formas específicas). Podemos:\n",
        "\n",
        "- Utilizá-los diretamente para classificação em tarefas semelhantes àquelas de treino;\n",
        "- Usá-los como extratores de características, adicionando novas camadas ao final (técnica conhecida como *transfer learning*).\n",
        "\n",
        "Nesta parte do notebook, vamos carregar e utilizar uma rede pré-treinada famosa para observar, na prática, como ela classifica imagens reais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_bD_cJfnrTT"
      },
      "source": [
        "### A arquitetura ResNet50\n",
        "\n",
        "Aqui vamos carregar a **ResNet50**, uma rede convolucional profunda composta por 50 camadas treinadas no dataset ImageNet. A grande inovação da família ResNet é o uso de conexões de atalho (*skip connections*), que ajudam a evitar problemas de desaparecimento do gradiente em redes muito profundas.\n",
        "\n",
        "Ao carregar `ResNet50(weights=\"imagenet\")`, estamos trazendo um modelo que já “viu” mais de um milhão de imagens e aprendeu a reconhecer mil classes diferentes. Nosso objetivo nesta aula não é treinar essa rede, e sim aproveitar o modelo já treinado para fazer classificações em imagens de exemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbS9p1FnpKz7"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.applications.ResNet50(weights=\"imagenet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0rXRzs-kigm"
      },
      "source": [
        "**Aviso**: A expressão `load_sample_images()[\"images\"]` retorna uma lista de imagens em Python. No entanto, nas versões mais recentes, o Keras não aceita mais listas em Python, então precisamos converter essa lista em um tensor. Podemos fazer isso usando `tf.constant()`, mas aqui usei `K.constant()`: ele simplesmente chama `tf.constant()` se você estiver usando o TensorFlow como backend (como é o caso aqui), mas se você decidir usar JAX ou PyTorch como backend, `K.constant()` chamará a função apropriada do backend escolhido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Qbj71MriTT"
      },
      "source": [
        "### Preparando as imagens para a ResNet50\n",
        "\n",
        "Modelos pré-treinados como a ResNet50 esperam que as imagens de entrada tenham um formato específico. No caso da ResNet50:\n",
        "\n",
        "- As imagens precisam ter tamanho 224×224 pixels;\n",
        "- Devem ter 3 canais de cor (RGB);\n",
        "- Passam por um pré-processamento próprio, definido na implementação da Keras.\n",
        "\n",
        "Nesta célula, vamos:\n",
        "\n",
        "- Carregar algumas imagens de exemplo;\n",
        "- Redimensioná-las para 224×224;\n",
        "- Organizar o tensor de forma adequada para ser enviado ao modelo.\n",
        "\n",
        "Esse cuidado com o pré-processamento é fundamental: se o formato da imagem não estiver compatível com o modelo, a rede não consegue aproveitar corretamente o conhecimento que adquiriu durante o treinamento no ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QhYKi22pKz8"
      },
      "outputs": [],
      "source": [
        "K = tf.keras.backend\n",
        "images = K.constant(load_sample_images()[\"images\"])\n",
        "images_resized = tf.keras.layers.Resizing(height=224, width=224,\n",
        "                                          crop_to_aspect_ratio=True)(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usbPpqkqpKz8"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-IYqzqRpKz8"
      },
      "outputs": [],
      "source": [
        "Y_proba = model.predict(inputs)\n",
        "Y_proba.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uWvslEcpKz8"
      },
      "outputs": [],
      "source": [
        "top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
        "for image_index in range(len(images)):\n",
        "    print(f\"Image #{image_index}\")\n",
        "    for class_id, name, y_proba in top_K[image_index]:\n",
        "        print(f\"  {class_id} - {name:12s} {y_proba:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alc_cnxVpKz8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for idx in (0, 1):\n",
        "    plt.subplot(1, 2, idx + 1)\n",
        "    plt.imshow(images_resized[idx] / 255)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkkAPqgzrnRN"
      },
      "source": [
        "## Exercício prático: classificando imagens com um modelo pré-treinado\n",
        "\n",
        "Neste exercício, você vai experimentar na prática o uso de um modelo pré-treinado para classificação de imagens.\n",
        "\n",
        "**Tarefa:**  \n",
        "Escolha **três imagens reais** (podem ser fotos do seu celular ou imagens da internet) contendo objetos bem definidos, como animais, veículos, alimentos, ferramentas ou outros itens do dia a dia. Em seguida:\n",
        "\n",
        "1. Carregue essas imagens no notebook;\n",
        "2. Faça o pré-processamento necessário (redimensionamento e normalização) de acordo com o modelo pré-treinado escolhido (por exemplo, ResNet50);\n",
        "3. Use o modelo para obter as previsões;\n",
        "4. Registre, para cada imagem:\n",
        "   - As classes previstas e suas probabilidades;\n",
        "   - Se o resultado faz sentido ou não;\n",
        "   - Possíveis motivos para erros ou confusões (ângulo da foto, iluminação, objeto muito diferente do que a rede “espera” etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b95B1Nrrrn87"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "model =\n",
        "\n",
        "img_path = \"sua_imagem.jpg\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "print(decode_predictions(preds, top=3)[0])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
